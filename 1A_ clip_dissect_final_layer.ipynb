{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FwDou6qSsSvE",
   "metadata": {
    "id": "FwDou6qSsSvE"
   },
   "source": [
    "# CLIP-Dissect\n",
    "\n",
    "Keywords: Neuron-level Interpretability, Network Dissection\n",
    "\n",
    "Link to paper: https://arxiv.org/abs/2204.10965\n",
    "\n",
    "About the datasets:\n",
    "\n",
    "- CIFAR100: standard dataset which contains 60k RGB images of size  32Ã—32  belonging to 100 classes of general objects and animals.\n",
    "\n",
    "- Places365: A scene recognition dataset with 365 classes: http://places.csail.mit.edu/\n",
    "\n",
    "- Broden: A diverse dataset for probing neurons with some overlap with Places365, introduced in Network Dissection http://netdissect.csail.mit.edu/.  \n",
    "\n",
    "Notes:\n",
    "- Make sure to enable GPU: Runtime -> Change runtime type -> Hardware accelerator: GPPU\n",
    "- Free Colab RAM is not large enough to run all experiments but should be enough for these. Sometimes restarting helps if it crashes due to RAM constraints.\n",
    "- Run all the cells in order.\n",
    "- Do not edit the cells marked with !!DO NOT EDIT!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52ee82-e7e5-4bb9-a1a8-efc6e009c98f",
   "metadata": {
    "id": "8e52ee82-e7e5-4bb9-a1a8-efc6e009c98f"
   },
   "source": [
    "## Quantitative evaluation for final layer neurons\n",
    "\n",
    "This notebook generates descriptions for neurons in the final(classification) layer of the neural network. We then compare how well they match the ground truth (name of the class the neuron corresponds to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c7be9-8a6a-44dd-89cb-877dffe8d4db",
   "metadata": {
    "id": "e29c7be9-8a6a-44dd-89cb-877dffe8d4db"
   },
   "outputs": [],
   "source": [
    "# !!DO NOT EDIT!!\n",
    "# run this only once per runtime\n",
    "import os\n",
    "!git clone https://github.com/Trustworthy-ML-Lab/CLIP-dissect\n",
    "os.chdir('CLIP-dissect')\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9vHFkfFJ2Zlj",
   "metadata": {
    "id": "9vHFkfFJ2Zlj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import clip\n",
    "import utils\n",
    "import data_utils\n",
    "import similarity\n",
    "import clip\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffKDAHNzmJtw",
   "metadata": {
    "id": "ffKDAHNzmJtw"
   },
   "outputs": [],
   "source": [
    "#Downloads Broden dataset and ResNet-18 trained on Places\n",
    "!bash dlbroden.sh\n",
    "!bash dlzoo_example.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e693e37-a3ae-4aad-a03d-492e2a08eb5d",
   "metadata": {
    "id": "0e693e37-a3ae-4aad-a03d-492e2a08eb5d"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc79d69-08a0-4236-b26b-45918aa7b108",
   "metadata": {
    "id": "abc79d69-08a0-4236-b26b-45918aa7b108"
   },
   "outputs": [],
   "source": [
    "clip_name = 'ViT-B/16'\n",
    "d_probe = 'broden'\n",
    "concept_set = 'data/broden_labels_clean.txt'\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "save_dir = 'saved_activations'\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724590a-2333-4daa-9948-6be1dfc60c25",
   "metadata": {
    "id": "1724590a-2333-4daa-9948-6be1dfc60c25"
   },
   "outputs": [],
   "source": [
    "target_name = 'resnet18_places'\n",
    "target_layer = 'fc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1eb6d-87cc-4430-b8e4-cd48d2643c7d",
   "metadata": {
    "id": "89f1eb6d-87cc-4430-b8e4-cd48d2643c7d"
   },
   "source": [
    "## Run CLIP-Dissect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a1e91-5363-43a3-8f0b-4a034515923e",
   "metadata": {
    "id": "4b6a1e91-5363-43a3-8f0b-4a034515923e"
   },
   "outputs": [],
   "source": [
    "utils.save_activations(clip_name = clip_name, target_name = target_name, target_layers = [target_layer],\n",
    "                       d_probe = d_probe, concept_set = concept_set, batch_size = batch_size,\n",
    "                       device = device, pool_mode=pool_mode, save_dir = save_dir)\n",
    "\n",
    "with open(concept_set, 'r') as f:\n",
    "    words = (f.read()).split('\\n')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0e205-0b81-4d59-80ac-16b321e56949",
   "metadata": {
    "id": "edd0e205-0b81-4d59-80ac-16b321e56949"
   },
   "outputs": [],
   "source": [
    "save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "\n",
    "target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "similarities, target_feats = utils.get_similarity_from_activations(target_save_name, clip_save_name,\n",
    "                                                                text_save_name, similarity_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o97E5Xnoyh1f",
   "metadata": {
    "id": "o97E5Xnoyh1f"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "clip_model, _ = clip.load(clip_name, device=device)\n",
    "\n",
    "with open('data/categories_places365.txt', 'r') as f:\n",
    "    classnames = (f.read()).split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qbyot8j_znKM",
   "metadata": {
    "id": "qbyot8j_znKM"
   },
   "source": [
    "# Collect results of CLIP-Dissect and baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TldX2b95zVjY",
   "metadata": {
    "id": "TldX2b95zVjY"
   },
   "outputs": [],
   "source": [
    "clip_preds = torch.argmax(similarities, dim=1)\n",
    "clip_preds = [words[int(pred)] for pred in clip_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0UVvDa9Nza6T",
   "metadata": {
    "id": "0UVvDa9Nza6T"
   },
   "outputs": [],
   "source": [
    "netdissect_res = pd.read_csv('data/NetDissect_results/resnet18_places365_fc.csv')\n",
    "nd_preds = netdissect_res['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9fb5f-4225-428b-b2f5-3b72ce3833b0",
   "metadata": {
    "id": "04e9fb5f-4225-428b-b2f5-3b72ce3833b0"
   },
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DPwsilDyzhH7",
   "metadata": {
    "id": "DPwsilDyzhH7"
   },
   "outputs": [],
   "source": [
    "#most activating images for each neuron\n",
    "top_vals, top_ids = torch.topk(target_feats, k=5, dim=0)\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)\n",
    "\n",
    "ids_to_check = [0, 1, 2]\n",
    "\n",
    "for orig_id in ids_to_check:#range(20):\n",
    "    orig_id = int(orig_id)\n",
    "    #print(mse)\n",
    "    print('\\n Layer:{} Neuron:{}, Gt:{}'.format(target_layer, orig_id, classnames[orig_id]))\n",
    "\n",
    "    clip_cos, mpnet_cos = utils.get_cos_similarity(nd_preds[orig_id:orig_id+1],\n",
    "                                                   classnames[orig_id:orig_id+1],\n",
    "                                                   clip_model, model, device, batch_size)\n",
    "    print('Network Dissection: {} {:.4f} {:.4f}'.format(nd_preds[orig_id], clip_cos, mpnet_cos))\n",
    "\n",
    "    clip_cos, mpnet_cos = utils.get_cos_similarity(clip_preds[orig_id:orig_id+1],\n",
    "                                                  classnames[orig_id:orig_id+1],\n",
    "                                                  clip_model, model, device, batch_size)\n",
    "    print('CLIP-Dissect: {} {:.4f} {:.4f}'.format(clip_preds[orig_id], clip_cos, mpnet_cos))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        im, label = pil_data[top_id]\n",
    "        im = im.resize([375,375])\n",
    "        fig.add_subplot(1, 5, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.title('Layer:{} Neuron:{}'.format(target_layer, (int(orig_id))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5yGzTDUZoOdu",
   "metadata": {
    "id": "5yGzTDUZoOdu"
   },
   "source": [
    "# Problems\n",
    "\n",
    "Complete the following tasks and record the results in Google Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4iJiS9SvmiPF",
   "metadata": {
    "id": "4iJiS9SvmiPF"
   },
   "source": [
    "**TASK 1:** Compare explanations for 10 different final layer neurons (hint: only need to rerun last cell).\n",
    "- Which explanations were better, Network Dissection or CLIP-Dissect? Does the highest cosine similarity to ground truth neuron correspond to good match based on your intuition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cmpvhUnZoyOd",
   "metadata": {
    "id": "cmpvhUnZoyOd"
   },
   "source": [
        "clip_cos, mpnet_cos = utils.get_cos_similarity(nd_preds[orig_id:orig_id+1],\n",
    "                                                   classnames[orig_id:orig_id+1],\n",
    "                                                   clip_model, model, device, batch_size)\n",
    "    print('Network Dissection: {} {:.4f} {:.4f}'.format(nd_preds[orig_id], clip_cos, mpnet_cos))\n",
    "\n",
    "    clip_cos, mpnet_cos = utils.get_cos_similarity(clip_preds[orig_id:orig_id+1],\n",
    "                                                  classnames[orig_id:orig_id+1],\n",
    "                                                  clip_model, model, device, batch_size)\n",
    "    print('CLIP-Dissect: {} {:.4f} {:.4f}'.format(clip_preds[orig_id], clip_cos, mpnet_cos))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        im, label = pil_data[top_id]\n",
    "        im = im.resize([375,375])\n",
    "        fig.add_subplot(1, 5, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.title('Layer:{} Neuron:{}'.format(target_layer, (int(orig_id))))\n",
    "    plt.show()"


   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKal6bGPrJVO",
   "metadata": {
    "id": "FKal6bGPrJVO"
   },
   "source": [
    "**TASK 3:** Vary other parameters and evaluate how it changes the results. Some possibilities:\n",
    "- Different similarity functions: see similarity.py for options\n",
    "- Different concept sets: for example try data/3k.txt or data/imagenet_labels (10k.txt and 20k.txt might run out of RAM in Colab)\n",
    "- Change probing dataset to CIFAR100_train (note: this only changes probing data for CLIP-Dissect, Network Dissection results are precalculated using Broden)\n",
    "- Extra: Can you postprocess classnames to be more readable without / etc. How does this change cos similarity numbers?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
