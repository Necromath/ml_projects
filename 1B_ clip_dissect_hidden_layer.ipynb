{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FwDou6qSsSvE",
   "metadata": {
    "id": "FwDou6qSsSvE"
   },
   "source": [
    "# CLIP-Dissect\n",
    "\n",
    "Keywords: Neuron-level Interpretability, Network Dissection\n",
    "\n",
    "Link to paper: https://arxiv.org/abs/2204.10965\n",
    "\n",
    "About the datasets:\n",
    "\n",
    "- CIFAR100: standard dataset which contains 60k RGB images of size  32Ã—32  belonging to 100 classes of general objects and animals.\n",
    "\n",
    "- Places365: A scene recognition dataset with 365 classes: http://places.csail.mit.edu/\n",
    "\n",
    "- Broden: A diverse dataset for probing neurons with some overlap with Places365, introduced in Network Dissection http://netdissect.csail.mit.edu/.  \n",
    "\n",
    "Notes:\n",
    "- Make sure to enable GPU: Runtime -> Change runtime type -> Hardware accelerator: GPPU\n",
    "- Free Colab RAM is not large enough to run all experiments but should be enough for these. Sometimes restarting helps if it crashes due to RAM constraints.\n",
    "- Run all the cells in order.\n",
    "- Do not edit the cells marked with !!DO NOT EDIT!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52ee82-e7e5-4bb9-a1a8-efc6e009c98f",
   "metadata": {
    "id": "8e52ee82-e7e5-4bb9-a1a8-efc6e009c98f"
   },
   "source": [
    "## Qualitative evaluation for hidden layers\n",
    "\n",
    "This notebook generates descriptions for neurons in the hidden layers of a neural network. Neuron descriptions are shown together with 5 most highly activating images for that neuron to evaluate their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c7be9-8a6a-44dd-89cb-877dffe8d4db",
   "metadata": {
    "id": "e29c7be9-8a6a-44dd-89cb-877dffe8d4db"
   },
   "outputs": [],
   "source": [
    "# !!DO NOT EDIT!!\n",
    "!git clone https://github.com/Trustworthy-ML-Lab/CLIP-dissect\n",
    "!pip install ftfy regex\n",
    "import os\n",
    "os.chdir('CLIP-dissect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fGPH4glM3yuo",
   "metadata": {
    "id": "fGPH4glM3yuo"
   },
   "outputs": [],
   "source": [
    "#Downloads Broden dataset and ResNet-18 trained on Places\n",
    "!bash dlbroden.sh\n",
    "!bash dlzoo_example.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NhxwOjn63waD",
   "metadata": {
    "id": "NhxwOjn63waD"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils\n",
    "import data_utils\n",
    "import similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e693e37-a3ae-4aad-a03d-492e2a08eb5d",
   "metadata": {
    "id": "0e693e37-a3ae-4aad-a03d-492e2a08eb5d"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc79d69-08a0-4236-b26b-45918aa7b108",
   "metadata": {
    "id": "abc79d69-08a0-4236-b26b-45918aa7b108"
   },
   "outputs": [],
   "source": [
    "clip_name = 'ViT-B/16'\n",
    "d_probe = 'cifar100_train'\n",
    "concept_set = 'data/3k.txt'\n",
    "batch_size = 200\n",
    "device = 'cuda'\n",
    "pool_mode = 'avg'\n",
    "\n",
    "save_dir = 'saved_activations'\n",
    "similarity_fn = similarity.soft_wpmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724590a-2333-4daa-9948-6be1dfc60c25",
   "metadata": {
    "id": "1724590a-2333-4daa-9948-6be1dfc60c25"
   },
   "outputs": [],
   "source": [
    "target_name = 'resnet18_places'\n",
    "target_layer = 'layer4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1eb6d-87cc-4430-b8e4-cd48d2643c7d",
   "metadata": {
    "id": "89f1eb6d-87cc-4430-b8e4-cd48d2643c7d"
   },
   "source": [
    "## Run CLIP-Dissect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a1e91-5363-43a3-8f0b-4a034515923e",
   "metadata": {
    "id": "4b6a1e91-5363-43a3-8f0b-4a034515923e"
   },
   "outputs": [],
   "source": [
    "utils.save_activations(clip_name = clip_name, target_name = target_name, target_layers = [target_layer],\n",
    "                       d_probe = d_probe, concept_set = concept_set, batch_size = batch_size,\n",
    "                       device = device, pool_mode=pool_mode, save_dir = save_dir)\n",
    "\n",
    "with open(concept_set, 'r') as f:\n",
    "    words = (f.read()).split('\\n')\n",
    "\n",
    "pil_data = data_utils.get_data(d_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0e205-0b81-4d59-80ac-16b321e56949",
   "metadata": {
    "id": "edd0e205-0b81-4d59-80ac-16b321e56949"
   },
   "outputs": [],
   "source": [
    "save_names = utils.get_save_names(clip_name = clip_name, target_name = target_name,\n",
    "                                  target_layer = target_layer, d_probe = d_probe,\n",
    "                                  concept_set = concept_set, pool_mode=pool_mode,\n",
    "                                  save_dir = save_dir)\n",
    "\n",
    "target_save_name, clip_save_name, text_save_name = save_names\n",
    "\n",
    "similarities, target_feats = utils.get_similarity_from_activations(target_save_name, clip_save_name,\n",
    "                                                                text_save_name, similarity_fn, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9fb5f-4225-428b-b2f5-3b72ce3833b0",
   "metadata": {
    "id": "04e9fb5f-4225-428b-b2f5-3b72ce3833b0"
   },
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae62e80-35ce-4301-b015-cd68fe4f6183",
   "metadata": {
    "id": "3ae62e80-35ce-4301-b015-cd68fe4f6183"
   },
   "outputs": [],
   "source": [
    "top_vals, top_ids = torch.topk(target_feats, k=5, dim=0)\n",
    "#neurons_to_check = torch.sort(torch.max(similarities, dim=1)[0], descending=True)[1][0:20]\n",
    "neurons_to_check = range(10)\n",
    "font_size = 14\n",
    "font = {'size'   : font_size}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "fig = plt.figure(figsize=[10, len(neurons_to_check)*2])#constrained_layout=True)\n",
    "subfigs = fig.subfigures(nrows=len(neurons_to_check), ncols=1)\n",
    "for j, orig_id in enumerate(neurons_to_check):\n",
    "    vals, ids = torch.topk(similarities[orig_id], k=5, largest=True)\n",
    "\n",
    "    subfig = subfigs[j]\n",
    "    subfig.text(0.13, 0.96, \"Neuron {}:\".format(int(orig_id)), size=font_size)\n",
    "    subfig.text(0.27, 0.96, \"CLIP-Dissect: {}, {:.2f}\".format(words[int(ids[0])], vals[0]), size=font_size)\n",
    "    #subfig.text(0.4, 0.96, words[int(ids[0])], size=font_size)\n",
    "    axs = subfig.subplots(nrows=1, ncols=5)\n",
    "    for i, top_id in enumerate(top_ids[:, orig_id]):\n",
    "        im, label = pil_data[top_id]\n",
    "        im = im.resize([375,375])\n",
    "        axs[i].imshow(im)\n",
    "        axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5yGzTDUZoOdu",
   "metadata": {
    "id": "5yGzTDUZoOdu"
   },
   "source": [
    "# To do\n",
    "\n",
    "Complete the following tasks and record the results in Google Slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4iJiS9SvmiPF",
   "metadata": {
    "id": "4iJiS9SvmiPF"
   },
   "source": [
    "**TASK 1:** Change neurons_to_check to a different set of neurons of your choosing, visualize and save the results (hint: only need to rerun last cell).\n",
    "- Keep the list of neurons and output image for later comparisons.\n",
    "- Evaluate whether the description of the neurons match the highly activating images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cmpvhUnZoyOd",
   "metadata": {
    "id": "cmpvhUnZoyOd"
   },
   "source": [
    "**TASK 2:** Change d_probe to 'broden'. Evaluate again with the same neurons. Are the concepts similar to before? Which dataset gives better matching concepts and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FKal6bGPrJVO",
   "metadata": {
    "id": "FKal6bGPrJVO"
   },
   "source": [
    "**TASK 3:** Vary other parameters and evaluate how it changes the results. Some possibilities:\n",
    "- Different layers: try 'conv1', 'layer1', 'layer2' or 'layer3'\n",
    "- Different similarity functions: see similarity.py for options\n",
    "- Different concept sets: for example try broden_labels_clean.txt (10k.txt and 20k.txt might run out of RAM in Colab)\n",
    "- Look at different models, for example 'resnet50' or 'resnet101' (by default loads an ImageNet trained model from torchvision)\n",
    "- Modify code to display top-k best descriptions for a neuron"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
